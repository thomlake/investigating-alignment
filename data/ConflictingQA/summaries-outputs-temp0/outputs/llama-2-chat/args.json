{
  "run": {
    "instance_file": "./data/ConflictingQA/summaries-outputs-temp0/instances/llama-2-chat/data.jsonl",
    "output_dir": "./data/ConflictingQA/summaries-outputs-temp0/outputs/llama-2-chat",
    "user_template_file": null,
    "user_template": null
  },
  "openai_params": {
    "model": "gpt-4-1106-preview",
    "temperature": 0,
    "response_format": null,
    "n": 1,
    "max_tokens": 256
  },
  "side_info_args": null
}